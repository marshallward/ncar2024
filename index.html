<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Marshall Ward">
  <meta name="dcterms.date" content="2024-08-28">
  <title>MOM6 on the GPU?</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="./reveal.js/dist/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="./reveal.js/css/theme/gfdl.css" id="theme">
  <!-- Explicitly add zenburn for highlight support -->
  <link rel="stylesheet" href="./reveal.js/plugin/highlight/zenburn.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? './reveal.js/css/print/pdf.scss' : './reveal.js/css/print/paper.scss';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="./reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <base href="./index.html">
</head>
<body>
  <div class="reveal"
       style="background: url(img/bg_gfdl.jpg);
              background-size: cover;">

    <header style="width: 10vh; position: absolute; bottom: 2vh; right: 2vh;">
      <!-- img src="img/noaa_logo.png" -->
    </header>

    <footer style="font-size: 1pc; position: absolute; bottom: 2%; left: 2%;">
      <code><p><a
href="https://github.com/marshallward/ncar2024.html">https://github.com/marshallward/ncar2024.html</a></p></code>
    </footer>

    <div class="slides">

<section id="title-slide">
  <!--div class="reveal" style="text-align: right;">
    <img src="img/noaa_logo.png"
         style="background: none; border: none; box-shadow: none;
         width: 30%"
         alt="NCI">
  </div-->
  <h1 class="title">MOM6 on the GPU?</h1>
  <p class="author" style="text-align: right;">Marshall Ward</p>
  <!-- org has a <p> for some reason... so use <div> -->
  <div class="organization" style="text-align: right;"><p>NOAA-GFDL</p></div>
  <p class="date" style="text-align: right;">2024-08-28</p>
  <!-- Currently cannot add notes to a title slide, so have to do manually-->
  <aside class="notes">
    <p>Gathering of talking points for porting MOM6 to GPU
    platforms.</p>
  </aside>
</section>

<section id="questions" class="title-slide slide level1">
<h1>Questions</h1>
<ul>
<li>GPU vs CPU: What do we gain?</li>
<li>How to produce GPU code from Fortran?</li>
<li>MOM6 as a community model</li>
</ul>
</section>

<section id="compute-vs-memory" class="title-slide slide level1">
<h1>Compute vs Memory</h1>
<p><img data-src="img/dgemm_vs_daxpy.svg" alt="image" /></p>
</section>

<section id="where-is-mom6" class="title-slide slide level1">
<h1>Where is MOM6?</h1>
<p><img data-src="img/mom6_roofline.svg" style="width:70.0%"
alt="image" /></p>
<p>More memory bound (like most models)</p>
</section>

<section>
<section id="peak-performance" class="title-slide slide level1">
<h1>Peak performance</h1>
<table>
<tbody>
<tr class="odd">
<td><p>Nvidia GPUs</p>
<table>
<thead>
<tr class="header">
<th>Arch</th>
<th>Year</th>
<th>TFLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>V100</td>
<td>2017</td>
<td>7.0</td>
</tr>
<tr class="even">
<td>A100</td>
<td>2020</td>
<td>9.7</td>
</tr>
<tr class="odd">
<td>H100</td>
<td>2022</td>
<td>25.6</td>
</tr>
</tbody>
</table></td>
<td><p>Intel CPUs</p>
<table>
<thead>
<tr class="header">
<th>Xeon</th>
<th>Year</th>
<th>TFLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8274</td>
<td>2019</td>
<td>2.1</td>
</tr>
<tr class="even">
<td>8368</td>
<td>2021</td>
<td>4.1</td>
</tr>
<tr class="odd">
<td>8592</td>
<td>2023</td>
<td>8.2</td>
</tr>
<tr class="even">
<td>6780</td>
<td>2024</td>
<td>13.8</td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
</section>
<section id="peak-gpu-nvidia" class="slide level2">
<h2>Peak GPU (Nvidia)</h2>
<p>Peak computation:</p>
<table>
<thead>
<tr class="header">
<th>Arch</th>
<th>Year</th>
<th>SMs</th>
<th>Cores</th>
<th>FMA</th>
<th>Clock (GHz)</th>
<th>TFLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>V100</td>
<td>2017</td>
<td>80</td>
<td>32</td>
<td>2</td>
<td>1.37</td>
<td>7.0</td>
</tr>
<tr class="even">
<td>A100</td>
<td>2020</td>
<td>108</td>
<td>32</td>
<td>2</td>
<td>1.41</td>
<td>9.7</td>
</tr>
<tr class="odd">
<td>H100</td>
<td>2022</td>
<td>114</td>
<td>64</td>
<td>2</td>
<td>1.755</td>
<td>25.6</td>
</tr>
</tbody>
</table>
<p>(2x with Tensor cores)</p>
</section>
<section id="peak-cpu-intel" class="slide level2">
<h2>Peak CPU (Intel)</h2>
<table>
<thead>
<tr class="header">
<th>CPU</th>
<th>Year</th>
<th>Cores</th>
<th>AVX</th>
<th>FMAc</th>
<th>Clock</th>
<th>TFLOPs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8274</td>
<td>2019</td>
<td>24</td>
<td>8</td>
<td>4</td>
<td>2.8</td>
<td>2.1</td>
</tr>
<tr class="even">
<td>8368</td>
<td>2021</td>
<td>38</td>
<td>8</td>
<td>4</td>
<td>3.4</td>
<td>4.1</td>
</tr>
<tr class="odd">
<td>8592</td>
<td>2023</td>
<td>64</td>
<td>8</td>
<td>4</td>
<td>4.0</td>
<td>8.2</td>
</tr>
<tr class="even">
<td>6780</td>
<td>2024</td>
<td>144</td>
<td>8</td>
<td>4</td>
<td>3.0</td>
<td>13.8</td>
</tr>
</tbody>
</table>
</section></section>
<section id="peak-bandwidth" class="title-slide slide level1">
<h1>Peak Bandwidth</h1>
<table>
<tbody>
<tr class="odd">
<td><p>Nvidia GPUs</p>
<table>
<thead>
<tr class="header">
<th>Arch</th>
<th>Year</th>
<th>GB/s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>V100</td>
<td>2017</td>
<td>896</td>
</tr>
<tr class="even">
<td>A100</td>
<td>2020</td>
<td>1555</td>
</tr>
<tr class="odd">
<td>H100</td>
<td>2022</td>
<td>2039</td>
</tr>
</tbody>
</table></td>
<td><p>Intel CPUs</p>
<table>
<thead>
<tr class="header">
<th>Xeon</th>
<th>Year</th>
<th>GB/s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8274</td>
<td>2019</td>
<td>141</td>
</tr>
<tr class="even">
<td>8368</td>
<td>2021</td>
<td>204</td>
</tr>
<tr class="odd">
<td>8592</td>
<td>2023</td>
<td>307</td>
</tr>
<tr class="even">
<td>6780</td>
<td>2024</td>
<td>409</td>
</tr>
</tbody>
</table></td>
</tr>
</tbody>
</table>
<p>GPU speedup due to HBM memory (vs DDR)</p>
</section>

<section id="what-should-we-gain" class="title-slide slide level1">
<h1>What should we gain?</h1>
<p>Ideally, we should expect <strong>5-10x speedup</strong>. But
performance is not the only reason to adopt GPUs:</p>
<ul>
<li>Economics: Costs of hardware and energy</li>
<li>Availability: Industry embrace of ML on GPU</li>
<li>Host/device design paradigm</li>
</ul>
</section>

<section>
<section id="paths-to-gpu-code" class="title-slide slide level1">
<h1>Paths to GPU code</h1>
<p><img data-src="img/gpucode.svg" alt="image" /></p>
<p>Red arrows show the Nvidia toolchain</p>
</section>
<section id="generating-gpu-bytecode" class="slide level2">
<h2>Generating GPU bytecode</h2>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p><img data-src="img/gpucode.svg" alt="image" /></p></td>
<td><p>Any language can use device APIs, but most only support CUDA.</p>
<p>Clang/Flang and GNU can target all device APIs. (Performance may
differ!)</p>
<p>Nvidia is a mostly <strong>closed</strong> ecosystem. OpenCL/HIP are
LLVM-based.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="gpu-api-sprawl" class="slide level2">
<h2>GPU API sprawl</h2>
<p><img data-src="img/2024-spirv-language-ecosystem.jpg"
alt="image" /></p>
<p>Each of these nodes is sitting within its own sprawling ecosystem</p>
</section>
<section id="under-the-hood" class="slide level2">
<h2>Under the Hood</h2>
<ul>
<li>CUDA: Nvidia proprietary language, API, and Runtime</li>
<li>OpenCL: Open alternative for Intel, AMD, and Nvidia</li>
<li>HIP: AMD-only alternative, part of AMD ROCm</li>
<li>SYCL is a newer cross-platform alternative (very C++)</li>
</ul>
<p>These can be rather low-level and often not easy to use. Probably
unrealistic to expect scientists to write at this level.</p>
</section>
<section id="even-deeper" class="slide level2">
<h2>Even deeper</h2>
<p>Assembly-like IRs, with some cross-vendor support:</p>
<ul>
<li>PTX (Nvidia): generated by CUDA, OpenCL, GCC(?)</li>
<li>SPIR (Intel): More general IR, compiled to either OpenCL driver or
Vulkan...?</li>
<li>HSAIL (AMD, ARM): generated from HIP, (...OpenCL?) via ROCm?
GCC?</li>
</ul>
<p>Although these are somewhat generic, for practical purposes they are
associated with particular hardware.</p>
</section></section>
<section id="fortran-options" class="title-slide slide level1">
<h1>Fortran Options</h1>
<dl>
<dt>OpenACC, OpenMP</dt>
<dd>
<p>Export loops to CUDA</p>
</dd>
<dt>Transpile to/from Fortran</dt>
<dd>
<p><a href="https://github.com/stfc/PSyclone">PSyclone</a>, PACE,
...</p>
</dd>
<dt>Hybrid programming</dt>
<dd>
<p>Fortran application, loops in CUDA cores</p>
</dd>
</dl>
<p>Others...?</p>
</section>

<section id="example-openacc" class="title-slide slide level1">
<h1>Example OpenACC</h1>
<p>Data transfers</p>
<pre class="fortran"><code>!$acc enter data copyin(CS%G)
!$acc enter data copyin(CS%G%dxT, CS%G%dyT, ...)
...
!$acc enter data copyout(u, v)</code></pre>
<p>Loop execution</p>
<pre class="fortran"><code>!$acc kernels present(G)
do k = 1,nz
   do j=1,n ; do i=1,n
      ...
   end do ; end do
   do j=1,n ; do i=1,n
      ...
   end do ; end do
enddo
!$acc end kernels</code></pre>
</section>

<section id="do-concurrent" class="title-slide slide level1">
<h1><code>do concurrent</code></h1>
<p>Arbitrary-order loops:</p>
<pre class="fortran"><code>do concurrent (i=1:m, j=1:n)
  a(i,j) = b(i,j) * c(i,j) + d(i,j)
end do</code></pre>
<p><a href="https://flang.llvm.org/docs/DoConcurrent.html">Not strictly
parallel</a> and subject to many internal constraints - but they can be
offloaded to GPUs.</p>
<p>(OpenACC can annotate <code>do concurrent</code> and overcome some of
these limitations.)</p>
</section>

<section>
<section id="openacc-in-mom6" class="title-slide slide level1">
<h1>OpenACC in MOM6</h1>
<ul>
<li>Parallelization often needs <code>private()</code></li>
<li>Data transfer often needs to be explicit
(<code>copyin/out</code>)</li>
<li>FMS inside loops:
<ul>
<li>halo updates (<code>pass_var</code>)</li>
<li>IO, diagnostics (<code>post_data</code>)</li>
<li>Clocks (<code>cpu_clock_begin</code>)</li>
<li>Error handling (<code>MOM_error</code>)</li>
</ul></li>
<li>165k statements (250k lines) to analyze</li>
</ul>
</section>
<section id="parallelization" class="slide level2">
<h2>Parallelization</h2>
<ol type="1">
<li><p>Ignore MPI for GPU builds? Assume PE == 1?</p></li>
<li><p>Consider MPI across nodes, GPU on-node?</p></li>
<li><p>Component parallelization (e.g. global barotropic on
GPU?)</p></li>
<li><p>Serious GPU runs need multiple cards.</p>
<p>How (if at all) do we integrate this into FMS
parallelization?</p></li>
</ol>
</section></section>
<section id="potential-targets" class="title-slide slide level1">
<h1>Potential Targets</h1>
<p><img data-src="img/mom6_time_table.svg" alt="image" /></p>
<p>Performance has a long tail distribution</p>
</section>

<section id="mom6-gpu-efforts" class="title-slide slide level1">
<h1>MOM6 GPU efforts</h1>
<ul>
<li>Barotropic solver (NCAR)</li>
<li>Vertical remapping workshop üòê</li>
<li>BGC do-concurrent migration (Niki Zadeh, GFDL)</li>
<li>Horizontal Viscosity (ongoing)</li>
</ul>
<p>Also ML-based solvers:</p>
<ul>
<li>PyTorch interface (Cheng Zhang; not yet merged)</li>
<li>SmartRedis interface (Andrew Shao)</li>
</ul>
</section>

<section>
<section id="mom6-consortium" class="title-slide slide level1">
<h1>MOM6 Consortium</h1>
<p><img data-src="img/consortium.svg" style="width:75.0%"
alt="image" /></p>
</section>
<section id="what-is-the-mom6-consortium" class="slide level2">
<h2>What is the MOM6 Consortium?</h2>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p><img data-src="img/consortium.svg" alt="image" /></p></td>
<td><p>Codebase is governed by a consortium of research groups.</p>
<p>All changes to the "hub" must be tracked and preserved.</p>
<p>Groups manage their own branch, and contribute to
<code>main</code>.</p></td>
</tr>
</tbody>
</table>
</section></section>
<section id="shared-development" class="title-slide slide level1">
<h1>Shared development</h1>
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 60%" />
</colgroup>
<tbody>
<tr class="odd">
<td><p><img data-src="img/git_nodes_dev.svg" alt="image" /></p></td>
<td><p>Many groups using diverse experiments and hardware</p>
<p>... but collaboration on a shared codebase.</p>
<p>This is achieved through a commitment to reproducibility.</p></td>
</tr>
</tbody>
</table>
</section>

<section id="bit-reproducibility" class="title-slide slide level1">
<h1>Bit reproducibility</h1>
<figure>
<video autoplay loop data-src="media/twolayer.mp4" controls=""><a
href="media/twolayer.mp4">media/twolayer.mp4</a></video>
</figure>
<p>We model over an enormous range of length and time scales, with
inherent turbulent cascades.</p>
</section>

<section id="when-to-reproduce" class="title-slide slide level1">
<h1>When to reproduce</h1>
<ul>
<li>Since each node has its own hardware, compilers, and configurations,
we cannot ever guarantee identical answers for everyone everyone.</li>
<li>Development must still go on, but we cannot violate each node's bit
reproducibility.</li>
<li>We achieve this by <em>self consistency</em>! What does this
mean?</li>
</ul>
</section>

<section>
<section id="self-consistency" class="title-slide slide level1">
<h1>Self Consistency</h1>
<dl>
<dt>Parallel Layout</dt>
<dd>
<p>All domain decompositions give the same result</p>
</dd>
<dt>Restart/Pickup</dt>
<dd>
<p>One <span class="math inline">\(\Delta t\)</span> run must equal two
<span class="math inline">\(\tfrac{1}{2}\Delta t\)</span> runs</p>
</dd>
<dt>Dimensionality</dt>
<dd>
<p>Equations must be invariant to dimensional scaling</p>
</dd>
<dt>Rotational invariance</dt>
<dd>
<p>Results must be invariant to an index rotation</p>
</dd>
</dl>
</section>
<section id="dimension-test" class="slide level2">
<h2>Dimension Test</h2>
<p><span class="math display">\[\begin{aligned}
u^{n+1} &amp;= u^{n} +  \Delta t \times \mathcal{F} \\
\color{yellow}{2^{L-T}} u^{n+1} &amp;= \color{yellow}{2^{L-T}} u^{n}
   + \color{yellow}{2^T} \Delta t
   \times \color{yellow}{2^{L - 2T}} \mathcal{F}
\end{aligned}\]</span></p>
<p>If equations are dimensionally correct, then solutions should be
invariant to scaling.</p>
</section>
<section id="rotation-test" class="slide level2">
<h2>Rotation Test</h2>
<p>Expressions should be invariant to rotation</p>
<table style="width:50%;">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<tbody>
<tr class="odd">
<td><img data-src="media/rotate.png" alt="image" /></td>
<td><img data-src="media/orig.png" alt="image" /></td>
</tr>
<tr class="even">
<td><img data-src="img/rotate_grid1.svg" alt="image" /></td>
<td><img data-src="img/rotate_grid2.svg" style="width:60.0%"
alt="image" /></td>
</tr>
</tbody>
</table>
</section></section>
<section id="fma-support" class="title-slide slide level1">
<h1>FMA support</h1>
<ul>
<li><p>Enabling FMAs will change answers, e.g. <span
class="math inline">\(a b + c d\)</span>. What is the order of
operations?</p></li>
<li><p><em>Self-consistent</em> FMAs are allowed, otherwise they are
disabled (e.g. rotation) with parentheses</p>
<pre><code>PFu(I,j,k) = &amp;
   (pa(i,j) * h(i,j,k) + intz_dpa(i,j)) &amp;
   - (pa(i+1,j) * h(i+1,j,k) + intz_dpa(i+1,j)) + ...</code></pre></li>
<li><p>FMA builds have their own (bitwise) answers</p></li>
<li><p>Can GPUs follow a similar pattern?</p></li>
</ul>
</section>

<section id="future-questions" class="title-slide slide level1">
<h1>Future questions</h1>
<ul>
<li>Continue with OpenACC? Or explore portability with OpenMP and
<code>do concurrent</code>?</li>
<li>How to restructure code to avoid FMS in loops?</li>
<li>Efficient memory management without maintenance overhead?</li>
<li>Can GPUs achieve any kind of self consistency? Or at least can
support GPUs while retaining CPU self consistency?</li>
</ul>
</section>
    </div>
  </div>

  <script src="./reveal.js/dist/reveal.js"></script>
  <script src="./reveal.js/plugin/math/math.js"></script>
  <script src="./reveal.js/plugin/notes/notes.js"></script>
  <script src="./reveal.js/plugin/highlight/highlight.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Display a presentation progress bar
        progress: true,
        // Display the page number of the current slide
        slideNumber: true,
        // Push each slide change to the browser history
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // Turns fragments on and off globally
        fragments: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Transition style
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        math: {
          //mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          "HTML-CSS": {
              preferredFont: "Neo-Euler",
              //preferredFont: "Asana-Math",
          },
          //TeX: {
          //  inlineMath: [['\\(','\\)']],
          //  displayMath: [['\\[','\\]']],
          //  balanceBraces: true,
          //  processEscapes: false,
          //  processRefs: true,
          //  processEnvironments: true,
          //  preview: 'TeX',
          //  skipTags: ['script','noscript','style','textarea','pre','code'],
          //  ignoreClass: 'tex2jax_ignore',
          //  processClass: 'tex2jax_process'
          //},
        },

        // Optional reveal.js plugins
        dependencies: [
          { src: './reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: './reveal.js/plugin/zoom/zoom.js', async: true },
          { src: './reveal.js/plugin/notes/notes.js', async: true }
        ],
        plugins : [ RevealMath, RevealNotes, RevealHighlight],
        pdfMaxPagesPerSlide: 1,
      });
    </script>
    </body>
</html>
