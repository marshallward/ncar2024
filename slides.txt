================
MOM6 on the GPU?
================

:author: Marshall Ward
:organization: NOAA-GFDL
:description: Thoughts on how MOM6 on a GPU may look (TODO)
:date: 2024-08-28
:url: https://github.com/marshallward/ncar2024.html
:preface:
   Gathering of talking points for porting MOM6 to GPU platforms.


Questions
=========

* GPU vs CPU: What do we gain?

* How to produce GPU code from Fortran?

* MOM6 as a community model


Compute vs Memory
=================

.. image:: img/dgemm_vs_daxpy.svg


Where is MOM6?
==============

.. image:: img/mom6_roofline.svg
   :width: 70%

More memory bound (like most models)


Peak performance
================

.. list-table::
   :width: 80 20

   * - Nvidia GPUs

       ====  ====  ======
       Arch  Year  TFLOPs
       ====  ====  ======
       V100  2017  7.0
       A100  2020  9.7
       H100  2022  25.6
       ====  ====  ======

     - Intel CPUs

       ====  ====  ======
       Xeon  Year  TFLOPs
       ====  ====  ======
       8274  2019  2.1
       8368  2021  4.1
       8592  2023  8.2
       6780  2024  13.8
       ====  ====  ======

.. NOTE FP64 speeds reported here, Tensor cores are 2x higher


Peak GPU (Nvidia)
-----------------

Peak computation:

====  ====  ====  ===== ===   =========== ======
Arch  Year  SMs   Cores FMA   Clock (GHz) TFLOPs
====  ====  ====  ===== ===   =========== ======
V100  2017  80    32    2     1.37        7.0
A100  2020  108   32    2     1.41        9.7
H100  2022  114   64    2     1.755       25.6
====  ====  ====  ===== ===   =========== ======

(2x with Tensor cores)

.. There are three V100 models, I chose the middle one.

.. H100 is for PCIe.  For SXM5, peak is 33.5 TFLOP/s

.. References
   https://resources.nvidia.com/en-us-tensor-core
   https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf


Peak CPU (Intel)
----------------

====  =====  =====  ===  ====  =====  ======
CPU   Year   Cores  AVX  FMAc  Clock  TFLOPs
====  =====  =====  ===  ====  =====  ======
8274  2019   24     8    4     2.8    2.1
8368  2021   38     8    4     3.4    4.1
8592  2023   64     8    4     4.0    8.2
6780  2024   144    8    4     3.0    13.8
====  =====  =====  ===  ====  =====  ======


Peak Bandwidth
==============

.. TODO: Compute these from specs, dont copy from Wikipedia

.. list-table::

   * - Nvidia GPUs

       ====  ====  ======
       Arch  Year  GB/s
       ====  ====  ======
       V100  2017  896
       A100  2020  1555
       H100  2022  2039
       ====  ====  ======

     - Intel CPUs

       ====  ====  ======
       Xeon  Year  GB/s
       ====  ====  ======
       8274  2019  141
       8368  2021  204
       8592  2023  307
       6780  2024  409
       ====  ====  ======

GPU speedup due to HBM memory (vs DDR)

.. PCIe speeds reported here, SXM is quite a bit higher


What should we gain?
====================

Ideally, we should expect **5-10x speedup**.  But performance is not the only
reason to adopt GPUs:

* Economics: Costs of hardware and energy

  .. slide on GPU vs CPU cost?  Power consumption

* Availability: Industry embrace of ML on GPU

  .. DoE has fallen in love with GPUs

* Host/device design paradigm


Paths to GPU code
=================

.. image:: img/gpucode.svg

Red arrows show the Nvidia toolchain


Generating GPU bytecode
-----------------------

.. list-table::
   :widths: 50 50

   * - .. image:: img/gpucode.svg

     - Any language can use device APIs, but most only support CUDA.

       Clang/Flang and GNU can target all device APIs. (Performance may
       differ!)

       Nvidia is a mostly **closed** ecosystem.  OpenCL/HIP are LLVM-based.


GPU API sprawl
--------------

.. image:: img/2024-spirv-language-ecosystem.jpg

Each of these nodes is sitting within its own sprawling ecosystem


Under the Hood
--------------

* CUDA: Nvidia proprietary language, API, and Runtime

* OpenCL: Open alternative for Intel, AMD, and Nvidia

* HIP: AMD-only alternative, part of AMD ROCm

* SYCL is a newer cross-platform alternative (very C++)

These can be rather low-level and often not easy to use.  Probably unrealistic
to expect scientists to write at this level.


Even deeper
-----------

Assembly-like IRs, with some cross-vendor support:

* PTX (Nvidia): generated by CUDA, OpenCL, GCC(?)

* SPIR (Intel): More general IR, compiled to either OpenCL driver or Vulkan...?

* HSAIL (AMD, ARM): generated from HIP, (...OpenCL?) via ROCm? GCC?

Although these are somewhat generic, for practical purposes they are associated
with particular hardware.


Fortran Options
===============

OpenACC, OpenMP
   Export loops to CUDA

Transpile to/from Fortran
   PSyclone_, PACE, ...

Hybrid programming
   Fortran application, loops in CUDA cores

Others...?

.. _PSyclone: https://github.com/stfc/PSyclone


Example OpenACC
===============

Data transfers

.. code:: fortran

   !$acc enter data copyin(CS%G)
   !$acc enter data copyin(CS%G%dxT, CS%G%dyT, ...)
   ...
   !$acc enter data copyout(u, v)

Loop execution

.. code:: fortran

   !$acc kernels present(G)
   do k = 1,nz
      do j=1,n ; do i=1,n
         ...
      end do ; end do
      do j=1,n ; do i=1,n
         ...
      end do ; end do
   enddo
   !$acc end kernels


.. OpenMP
   ======

   OpenMP 4.0 introduced the ``target`` construct:

   .. code:: fortran

      !$omp target data map(to:x) map(tofrom:y)
      do i = 1, n
        y(i) = a * x(i) + y(i)
      end do
      !$omp end target


   OpenACC and OpenMP
   ==================

   * Both are open standards, OpenACC has Nvidia connections

   * OpenACC host/target support predates OpenMP

   * Both standards heavily mirror each other, similar people driving development
     of each standard.

   .. NOAA and NCAR are auxiliary members, Daniel Howard is Corporate Officer


   OpenACC vs OpenMP
   =================

   * OpenACC has more features, such as the very convenient ``$acc kernels``

   * OpenMP has wider support across systems and compilers


``do concurrent``
=================

Arbitrary-order loops:

.. code:: fortran

   do concurrent (i=1:m, j=1:n)
     a(i,j) = b(i,j) * c(i,j) + d(i,j)
   end do

`Not strictly parallel`_ and subject to many internal constraints - but they
can be offloaded to GPUs.

.. _Not strictly parallel: https://flang.llvm.org/docs/DoConcurrent.html

(OpenACC can annotate ``do concurrent`` and overcome some of these
limitations.)

.. other notes
   GNU cannot even parse some do concurrent decorators, see below.
   https://gcc.gnu.org/bugzilla/show_bug.cgi?id=101602

.. Nvidia and Intel are both promoting this feature


OpenACC in MOM6
===============

* Parallelization often needs ``private()``

* Data transfer often needs to be explicit (``copyin/out``)

* FMS inside loops:

  * halo updates (``pass_var``)
  * IO, diagnostics (``post_data``)
  * Clocks (``cpu_clock_begin``)
  * Error handling (``MOM_error``)

* 165k statements (250k lines) to analyze


Parallelization
---------------

1. Ignore MPI for GPU builds?  Assume PE == 1?

2. Consider MPI across nodes, GPU on-node?

3. Component parallelization (e.g. global barotropic on GPU?)

4. Serious GPU runs need multiple cards.

   How (if at all) do we integrate this into FMS parallelization?

Potential Targets
=================

.. image:: img/mom6_time_table.svg

Performance has a long tail distribution


MOM6 GPU efforts
================

* Barotropic solver (NCAR)

* Vertical remapping workshop üòê

* BGC do-concurrent migration (Niki Zadeh, GFDL)

* Horizontal Viscosity (ongoing)

Also ML-based solvers:

* PyTorch interface (Cheng Zhang; not yet merged)

* SmartRedis interface (Andrew Shao)


.. Barriers to development
   =======================

   * Contributions from new scientists

   * Long-term maintenance

   * Portability across partners

   * Bit reproducibility across partners

.. -----

.. Path to Integration
   ===================

   TODO...
   (what does this even mean)

.. -----


MOM6 Consortium
===============

.. image:: img/consortium.svg
   :width: 75%


What is the MOM6 Consortium?
----------------------------

.. list-table::
   :widths: 50 50

   - * .. image:: img/consortium.svg

     * Codebase is governed by a consortium of research groups.

       All changes to the "hub" must be tracked and preserved.

       Groups manage their own branch, and contribute to ``main``.


Shared development
==================

.. list-table::
   :widths: 40 60

   - * .. image:: img/git_nodes_dev.svg

     * Many groups using diverse experiments and hardware

       ... but collaboration on a shared codebase.

       This is achieved through a commitment to reproducibility.


Bit reproducibility
===================

.. figure:: media/twolayer.mp4

We model over an enormous range of length and time scales, with inherent
turbulent cascades.


When to reproduce
=================

* Since each node has its own hardware, compilers, and configurations, we
  cannot ever guarantee identical answers for everyone everyone.

* Development must still go on, but we cannot violate each node's bit
  reproducibility.

* We achieve this by *self consistency*!  What does this mean?


Self Consistency
================

Parallel Layout
   All domain decompositions give the same result

Restart/Pickup
   One :math:`\Delta t` run must equal two :math:`\tfrac{1}{2}\Delta t` runs

Dimensionality
   Equations must be invariant to dimensional scaling

Rotational invariance
   Results must be invariant to an index rotation

.. Aggressive initialization
   NaN-initialization arrays vs. uninitialized arrays


Dimension Test
--------------

.. math::

   u^{n+1} &= u^{n} +  \Delta t \times \mathcal{F} \\
   \color{yellow}{2^{L-T}} u^{n+1} &= \color{yellow}{2^{L-T}} u^{n}
      + \color{yellow}{2^T} \Delta t
      \times \color{yellow}{2^{L - 2T}} \mathcal{F}

If equations are dimensionally correct, then solutions should be invariant to
scaling.


Rotation Test
-------------

Expressions should be invariant to rotation

.. list-table::
   :widths: 25 25 25 25

   * - .. image:: media/rotate.png

     - .. image:: media/orig.png

   * - .. image:: img/rotate_grid1.svg

     - .. image:: img/rotate_grid2.svg
          :width: 60%


FMA support
===========

* Enabling FMAs will change answers, e.g. :math:`a b + c d`.  What is the order
  of operations?

* *Self-consistent* FMAs are allowed, otherwise they are disabled (e.g.
  rotation) with parentheses

  .. code::

      PFu(I,j,k) = &
         (pa(i,j) * h(i,j,k) + intz_dpa(i,j)) &
         - (pa(i+1,j) * h(i+1,j,k) + intz_dpa(i+1,j)) + ...

* FMA builds have their own (bitwise) answers

* Can GPUs follow a similar pattern?


.. see https://github.com/marshallward/fma-op-order


Future questions
================

* Continue with OpenACC?  Or explore portability with OpenMP and ``do
  concurrent``?

* How to restructure code to avoid FMS in loops?

* Efficient memory management without maintenance overhead?

* Can GPUs achieve any kind of self consistency?  Or at least can support GPUs
  while retaining CPU self consistency?


.. Proposed Strategy
   =================

   1. Focus on Nvidia, start with OpenACC

      Nvidia has first-class support for OpenACC -> CUDA conversion, and we can
      expect the highest level of support.

      a. Embrace managed memory?  Or manual data transfer?

   2. Explore transition to OpenMP

      OpenMP and OpenACC mirror each other, with some nuances

      a. Extend to Intel? AMD?

   3. Embrace language-specific constructs

      ``do concurrent`` with OpenACC/OpenMP may allow for convergence of CPU and
      GPU code blocks.

   4. Focus on single-GPU then...

      a. 1 MPI rank per GPU?

      b. Single MPI rank, multiple GPUs?


.. 32-bit mode?
   ============

   * Ocean depth ~ 4km, dynamically siginificant depth is 10mm

   * Ocean tracers are conserved over centuries

     * Need more than 7 digits

   .. (Have I got that right?)

   * Momentum exchange with ocean is fast, single precision is sufficent

     * But is it worth the cost of constant conversion?
